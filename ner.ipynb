{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from bilm import Batcher, TokenBatcher, BidirectionalLanguageModel, weight_layers, dump_token_embeddings\n",
    "import com.capone.utils.data_utils as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/Users/yqk798/Downloads/datasets/elmo'\n",
    "vocab_file = os.path.join(datadir, 'vocab.txt')\n",
    "options_file = os.path.join(datadir, 'options.json')\n",
    "weight_file = os.path.join(datadir, 'lm_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Batcher to map text to character ids.\n",
    "batcher = Batcher(vocab_file, 50)\n",
    "token_batcher = TokenBatcher(vocab_file)\n",
    "\n",
    "# Input placeholders to the biLM.\n",
    "question_character_ids = tf.placeholder('int32', shape=(None, None, 50))\n",
    "question_token_ids = tf.placeholder('int32', shape=(None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding_file = os.path.join(datadir, 'elmo_token_embeddings.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/yqk798/Library/Python/3.6/lib/python/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-30 16:23:53,037 From /Users/yqk798/Library/Python/3.6/lib/python/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/bilm-0.1.post5-py3.6.egg/bilm/model.py:384: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-30 16:23:53,082 From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/bilm-0.1.post5-py3.6.egg/bilm/model.py:384: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING SKIP CONNECTIONS\n"
     ]
    }
   ],
   "source": [
    "dump_token_embeddings(\n",
    "    vocab_file, options_file, weight_file, token_embedding_file\n",
    ")\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the biLM graph.\n",
    "bilm = BidirectionalLanguageModel(options_file, weight_file)\n",
    "bilm_token = BidirectionalLanguageModel(\n",
    "    options_file,\n",
    "    weight_file,\n",
    "    use_character_inputs=False,\n",
    "    embedding_weight_file=token_embedding_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/yqk798/Library/Python/3.6/lib/python/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-03 11:49:25,378 From /Users/yqk798/Library/Python/3.6/lib/python/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING SKIP CONNECTIONS\n"
     ]
    }
   ],
   "source": [
    "#question_embeddings_op = bilm(question_character_ids)\n",
    "question_embeddings_op_token = bilm_token(question_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/bilm-0.1.post5-py3.6.egg/bilm/elmo.py:89: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-03 11:49:27,080 From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/bilm-0.1.post5-py3.6.egg/bilm/elmo.py:89: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('', tf.AUTO_REUSE):\n",
    "    # the reuse=True scope reuses weights from the context for the question\n",
    "    elmo_question_input = weight_layers(\n",
    "        'input', question_embeddings_op_token, l2_coef=0.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_question = [\n",
    "    ['what', 'are', 'biLMs', 'useful', 'for', '?', 'Varun'],\n",
    "    ['sure', 'useful']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     0    103     33 400002   4700     11    189 400002      0]\n",
      " [     0   1086   4700      0      0      0      0      0      0]]\n",
      "[[[ 0.7411803  -0.18728203  0.9542081  ...  0.46182865 -0.66130084\n",
      "    0.12696877]\n",
      "  [-0.5487164  -0.10489723 -0.00882029 ...  0.4861088   0.05831876\n",
      "    0.13532063]\n",
      "  [-1.2277577  -2.3290477   3.3235774  ...  0.9437058  -2.3616962\n",
      "    3.2876296 ]\n",
      "  ...\n",
      "  [-0.115685   -0.9261884  -0.6973815  ...  0.75920975 -0.6579843\n",
      "   -0.251489  ]\n",
      "  [-0.95735115 -1.3376005   2.327537   ...  0.8914468  -2.533528\n",
      "    2.483564  ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[-0.05013221 -0.18088299  0.2289171  ...  0.13237412 -0.22771375\n",
      "   -0.1142216 ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]]\n",
      "(2, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "# It is necessary to initialize variables once before running inference.\n",
    "question_ids = token_batcher.batch_sentences(tokenized_question)\n",
    "print(question_ids)\n",
    "elmo_question_input_ = sess.run(elmo_question_input['weighted_op'], \n",
    "                                feed_dict={question_token_ids: question_ids}\n",
    ")\n",
    "print(elmo_question_input_)\n",
    "print(elmo_question_input_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_train = du.CoNLLDataset('./data/ner/combined/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sents = []\n",
    "for (words, tags, _, _) in conll_train:\n",
    "    training_sents.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74954"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_lengths = [len(sent) for sent in training_sents]\n",
    "max_sent_len = max(sent_lengths)\n",
    "max_sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "BATCHES = len(training_sents) // BATCH_SIZE + (len(training_sents) % BATCH_SIZE != 0)\n",
    "print(BATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_array = np.zeros(shape=[len(training_sents), max_sent_len, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: 0\n",
      "BATCH: 1\n",
      "BATCH: 2\n",
      "BATCH: 3\n",
      "BATCH: 4\n",
      "BATCH: 5\n",
      "BATCH: 6\n",
      "BATCH: 7\n",
      "BATCH: 8\n",
      "BATCH: 9\n",
      "BATCH: 10\n",
      "BATCH: 11\n",
      "BATCH: 12\n",
      "BATCH: 13\n",
      "BATCH: 14\n",
      "BATCH: 15\n",
      "BATCH: 16\n",
      "BATCH: 17\n",
      "BATCH: 18\n",
      "BATCH: 19\n",
      "BATCH: 20\n",
      "BATCH: 21\n",
      "BATCH: 22\n",
      "BATCH: 23\n",
      "BATCH: 24\n",
      "BATCH: 25\n",
      "BATCH: 26\n",
      "BATCH: 27\n",
      "BATCH: 28\n",
      "BATCH: 29\n",
      "BATCH: 30\n",
      "BATCH: 31\n",
      "BATCH: 32\n",
      "BATCH: 33\n",
      "BATCH: 34\n",
      "BATCH: 35\n",
      "BATCH: 36\n",
      "BATCH: 37\n",
      "BATCH: 38\n",
      "BATCH: 39\n",
      "BATCH: 40\n",
      "BATCH: 41\n",
      "BATCH: 42\n",
      "BATCH: 43\n",
      "BATCH: 44\n",
      "BATCH: 45\n",
      "BATCH: 46\n",
      "BATCH: 47\n",
      "BATCH: 48\n",
      "BATCH: 49\n",
      "BATCH: 50\n",
      "BATCH: 51\n",
      "BATCH: 52\n",
      "BATCH: 53\n",
      "BATCH: 54\n",
      "BATCH: 55\n",
      "BATCH: 56\n",
      "BATCH: 57\n",
      "BATCH: 58\n",
      "BATCH: 59\n",
      "BATCH: 60\n",
      "BATCH: 61\n",
      "BATCH: 62\n",
      "BATCH: 63\n",
      "BATCH: 64\n",
      "BATCH: 65\n",
      "BATCH: 66\n",
      "BATCH: 67\n",
      "BATCH: 68\n",
      "BATCH: 69\n",
      "BATCH: 70\n",
      "BATCH: 71\n",
      "BATCH: 72\n",
      "BATCH: 73\n",
      "BATCH: 74\n",
      "BATCH: 75\n",
      "BATCH: 76\n",
      "BATCH: 77\n",
      "BATCH: 78\n",
      "BATCH: 79\n",
      "BATCH: 80\n",
      "BATCH: 81\n",
      "BATCH: 82\n",
      "BATCH: 83\n",
      "BATCH: 84\n",
      "BATCH: 85\n",
      "BATCH: 86\n",
      "BATCH: 87\n",
      "BATCH: 88\n",
      "BATCH: 89\n",
      "BATCH: 90\n",
      "BATCH: 91\n",
      "BATCH: 92\n",
      "BATCH: 93\n",
      "BATCH: 94\n",
      "BATCH: 95\n",
      "BATCH: 96\n",
      "BATCH: 97\n",
      "BATCH: 98\n",
      "BATCH: 99\n",
      "BATCH: 100\n",
      "BATCH: 101\n",
      "BATCH: 102\n",
      "BATCH: 103\n",
      "BATCH: 104\n",
      "BATCH: 105\n",
      "BATCH: 106\n",
      "BATCH: 107\n",
      "BATCH: 108\n",
      "BATCH: 109\n",
      "BATCH: 110\n",
      "BATCH: 111\n",
      "BATCH: 112\n",
      "BATCH: 113\n",
      "BATCH: 114\n",
      "BATCH: 115\n",
      "BATCH: 116\n",
      "BATCH: 117\n",
      "BATCH: 118\n",
      "BATCH: 119\n",
      "BATCH: 120\n",
      "BATCH: 121\n",
      "BATCH: 122\n",
      "BATCH: 123\n",
      "BATCH: 124\n",
      "BATCH: 125\n",
      "BATCH: 126\n",
      "BATCH: 127\n",
      "BATCH: 128\n",
      "BATCH: 129\n",
      "BATCH: 130\n",
      "BATCH: 131\n",
      "BATCH: 132\n",
      "BATCH: 133\n",
      "BATCH: 134\n",
      "BATCH: 135\n",
      "BATCH: 136\n",
      "BATCH: 137\n",
      "BATCH: 138\n",
      "BATCH: 139\n",
      "BATCH: 140\n",
      "BATCH: 141\n",
      "BATCH: 142\n",
      "BATCH: 143\n",
      "BATCH: 144\n",
      "BATCH: 145\n",
      "BATCH: 146\n",
      "BATCH: 147\n",
      "BATCH: 148\n",
      "BATCH: 149\n",
      "BATCH: 150\n",
      "BATCH: 151\n",
      "BATCH: 152\n",
      "BATCH: 153\n",
      "BATCH: 154\n",
      "BATCH: 155\n",
      "BATCH: 156\n",
      "BATCH: 157\n",
      "BATCH: 158\n",
      "BATCH: 159\n",
      "BATCH: 160\n",
      "BATCH: 161\n",
      "BATCH: 162\n",
      "BATCH: 163\n",
      "BATCH: 164\n",
      "BATCH: 165\n",
      "BATCH: 166\n",
      "BATCH: 167\n",
      "BATCH: 168\n",
      "BATCH: 169\n",
      "BATCH: 170\n",
      "BATCH: 171\n",
      "BATCH: 172\n",
      "BATCH: 173\n",
      "BATCH: 174\n",
      "BATCH: 175\n",
      "BATCH: 176\n",
      "BATCH: 177\n",
      "BATCH: 178\n",
      "BATCH: 179\n",
      "BATCH: 180\n",
      "BATCH: 181\n",
      "BATCH: 182\n",
      "BATCH: 183\n",
      "BATCH: 184\n",
      "BATCH: 185\n",
      "BATCH: 186\n",
      "BATCH: 187\n",
      "BATCH: 188\n",
      "BATCH: 189\n",
      "BATCH: 190\n",
      "BATCH: 191\n",
      "BATCH: 192\n",
      "BATCH: 193\n",
      "BATCH: 194\n",
      "BATCH: 195\n",
      "BATCH: 196\n",
      "BATCH: 197\n",
      "BATCH: 198\n",
      "BATCH: 199\n",
      "BATCH: 200\n",
      "BATCH: 201\n",
      "BATCH: 202\n",
      "BATCH: 203\n",
      "BATCH: 204\n",
      "BATCH: 205\n",
      "BATCH: 206\n",
      "BATCH: 207\n",
      "BATCH: 208\n",
      "BATCH: 209\n",
      "BATCH: 210\n",
      "BATCH: 211\n",
      "BATCH: 212\n",
      "BATCH: 213\n",
      "BATCH: 214\n",
      "BATCH: 215\n",
      "BATCH: 216\n",
      "BATCH: 217\n",
      "BATCH: 218\n",
      "BATCH: 219\n",
      "BATCH: 220\n",
      "BATCH: 221\n",
      "BATCH: 222\n",
      "BATCH: 223\n",
      "BATCH: 224\n",
      "BATCH: 225\n",
      "BATCH: 226\n",
      "BATCH: 227\n",
      "BATCH: 228\n",
      "BATCH: 229\n",
      "BATCH: 230\n",
      "BATCH: 231\n",
      "BATCH: 232\n",
      "BATCH: 233\n",
      "BATCH: 234\n",
      "BATCH: 235\n",
      "BATCH: 236\n",
      "BATCH: 237\n",
      "BATCH: 238\n",
      "BATCH: 239\n",
      "BATCH: 240\n",
      "BATCH: 241\n",
      "BATCH: 242\n",
      "BATCH: 243\n",
      "BATCH: 244\n",
      "BATCH: 245\n",
      "BATCH: 246\n",
      "BATCH: 247\n",
      "BATCH: 248\n",
      "BATCH: 249\n",
      "BATCH: 250\n",
      "BATCH: 251\n",
      "BATCH: 252\n",
      "BATCH: 253\n",
      "BATCH: 254\n",
      "BATCH: 255\n",
      "BATCH: 256\n",
      "BATCH: 257\n",
      "BATCH: 258\n",
      "BATCH: 259\n",
      "BATCH: 260\n",
      "BATCH: 261\n",
      "BATCH: 262\n",
      "BATCH: 263\n",
      "BATCH: 264\n",
      "BATCH: 265\n",
      "BATCH: 266\n",
      "BATCH: 267\n",
      "BATCH: 268\n",
      "BATCH: 269\n",
      "BATCH: 270\n",
      "BATCH: 271\n",
      "BATCH: 272\n",
      "BATCH: 273\n",
      "BATCH: 274\n",
      "BATCH: 275\n",
      "BATCH: 276\n",
      "BATCH: 277\n",
      "BATCH: 278\n",
      "BATCH: 279\n",
      "BATCH: 280\n",
      "BATCH: 281\n",
      "BATCH: 282\n",
      "BATCH: 283\n",
      "BATCH: 284\n",
      "BATCH: 285\n",
      "BATCH: 286\n",
      "BATCH: 287\n",
      "BATCH: 288\n",
      "BATCH: 289\n",
      "BATCH: 290\n",
      "BATCH: 291\n",
      "BATCH: 292\n",
      "BATCH: 293\n",
      "BATCH: 294\n",
      "BATCH: 295\n",
      "BATCH: 296\n",
      "BATCH: 297\n",
      "BATCH: 298\n",
      "BATCH: 299\n",
      "BATCH: 300\n",
      "BATCH: 301\n",
      "BATCH: 302\n",
      "BATCH: 303\n",
      "BATCH: 304\n",
      "BATCH: 305\n",
      "BATCH: 306\n",
      "BATCH: 307\n",
      "BATCH: 308\n",
      "BATCH: 309\n",
      "BATCH: 310\n",
      "BATCH: 311\n",
      "BATCH: 312\n",
      "BATCH: 313\n",
      "BATCH: 314\n",
      "BATCH: 315\n",
      "BATCH: 316\n",
      "BATCH: 317\n",
      "BATCH: 318\n",
      "BATCH: 319\n",
      "BATCH: 320\n",
      "BATCH: 321\n",
      "BATCH: 322\n",
      "BATCH: 323\n",
      "BATCH: 324\n",
      "BATCH: 325\n",
      "BATCH: 326\n",
      "BATCH: 327\n",
      "BATCH: 328\n",
      "BATCH: 329\n",
      "BATCH: 330\n",
      "BATCH: 331\n",
      "BATCH: 332\n",
      "BATCH: 333\n",
      "BATCH: 334\n",
      "BATCH: 335\n",
      "BATCH: 336\n",
      "BATCH: 337\n",
      "BATCH: 338\n",
      "BATCH: 339\n",
      "BATCH: 340\n",
      "BATCH: 341\n",
      "BATCH: 342\n",
      "BATCH: 343\n",
      "BATCH: 344\n",
      "BATCH: 345\n",
      "BATCH: 346\n",
      "BATCH: 347\n",
      "BATCH: 348\n",
      "BATCH: 349\n",
      "BATCH: 350\n",
      "BATCH: 351\n",
      "BATCH: 352\n",
      "BATCH: 353\n",
      "BATCH: 354\n",
      "BATCH: 355\n",
      "BATCH: 356\n",
      "BATCH: 357\n",
      "BATCH: 358\n",
      "BATCH: 359\n",
      "BATCH: 360\n",
      "BATCH: 361\n",
      "BATCH: 362\n",
      "BATCH: 363\n",
      "BATCH: 364\n",
      "BATCH: 365\n",
      "BATCH: 366\n",
      "BATCH: 367\n",
      "BATCH: 368\n",
      "BATCH: 369\n",
      "BATCH: 370\n",
      "BATCH: 371\n",
      "BATCH: 372\n",
      "BATCH: 373\n",
      "BATCH: 374\n",
      "BATCH: 375\n",
      "BATCH: 376\n",
      "BATCH: 377\n",
      "BATCH: 378\n",
      "BATCH: 379\n",
      "BATCH: 380\n",
      "BATCH: 381\n",
      "BATCH: 382\n",
      "BATCH: 383\n",
      "BATCH: 384\n",
      "BATCH: 385\n",
      "BATCH: 386\n",
      "BATCH: 387\n",
      "BATCH: 388\n",
      "BATCH: 389\n",
      "BATCH: 390\n",
      "BATCH: 391\n",
      "BATCH: 392\n",
      "BATCH: 393\n",
      "BATCH: 394\n",
      "BATCH: 395\n",
      "BATCH: 396\n",
      "BATCH: 397\n",
      "BATCH: 398\n",
      "BATCH: 399\n",
      "BATCH: 400\n",
      "BATCH: 401\n",
      "BATCH: 402\n",
      "BATCH: 403\n",
      "BATCH: 404\n",
      "BATCH: 405\n",
      "BATCH: 406\n",
      "BATCH: 407\n",
      "BATCH: 408\n",
      "BATCH: 409\n",
      "BATCH: 410\n",
      "BATCH: 411\n",
      "BATCH: 412\n",
      "BATCH: 413\n",
      "BATCH: 414\n",
      "BATCH: 415\n",
      "BATCH: 416\n",
      "BATCH: 417\n",
      "BATCH: 418\n",
      "BATCH: 419\n",
      "BATCH: 420\n",
      "BATCH: 421\n",
      "BATCH: 422\n",
      "BATCH: 423\n",
      "BATCH: 424\n",
      "BATCH: 425\n",
      "BATCH: 426\n",
      "BATCH: 427\n",
      "BATCH: 428\n",
      "BATCH: 429\n",
      "BATCH: 430\n",
      "BATCH: 431\n",
      "BATCH: 432\n",
      "BATCH: 433\n",
      "BATCH: 434\n",
      "BATCH: 435\n",
      "BATCH: 436\n",
      "BATCH: 437\n",
      "BATCH: 438\n",
      "BATCH: 439\n",
      "BATCH: 440\n",
      "BATCH: 441\n",
      "BATCH: 442\n",
      "BATCH: 443\n",
      "BATCH: 444\n",
      "BATCH: 445\n",
      "BATCH: 446\n",
      "BATCH: 447\n",
      "BATCH: 448\n",
      "BATCH: 449\n",
      "BATCH: 450\n",
      "BATCH: 451\n",
      "BATCH: 452\n",
      "BATCH: 453\n",
      "BATCH: 454\n",
      "BATCH: 455\n",
      "BATCH: 456\n",
      "BATCH: 457\n",
      "BATCH: 458\n",
      "BATCH: 459\n",
      "BATCH: 460\n",
      "BATCH: 461\n",
      "BATCH: 462\n",
      "BATCH: 463\n",
      "BATCH: 464\n",
      "BATCH: 465\n",
      "BATCH: 466\n",
      "BATCH: 467\n",
      "BATCH: 468\n",
      "BATCH: 469\n",
      "BATCH: 470\n",
      "BATCH: 471\n",
      "BATCH: 472\n",
      "BATCH: 473\n",
      "BATCH: 474\n",
      "BATCH: 475\n",
      "BATCH: 476\n",
      "BATCH: 477\n",
      "BATCH: 478\n",
      "BATCH: 479\n",
      "BATCH: 480\n",
      "BATCH: 481\n",
      "BATCH: 482\n",
      "BATCH: 483\n",
      "BATCH: 484\n",
      "BATCH: 485\n",
      "BATCH: 486\n",
      "BATCH: 487\n",
      "BATCH: 488\n",
      "BATCH: 489\n",
      "BATCH: 490\n",
      "BATCH: 491\n",
      "BATCH: 492\n",
      "BATCH: 493\n",
      "BATCH: 494\n",
      "BATCH: 495\n",
      "BATCH: 496\n",
      "BATCH: 497\n",
      "BATCH: 498\n",
      "BATCH: 499\n",
      "BATCH: 500\n",
      "BATCH: 501\n",
      "BATCH: 502\n",
      "BATCH: 503\n",
      "BATCH: 504\n",
      "BATCH: 505\n",
      "BATCH: 506\n",
      "BATCH: 507\n",
      "BATCH: 508\n",
      "BATCH: 509\n",
      "BATCH: 510\n",
      "BATCH: 511\n",
      "BATCH: 512\n",
      "BATCH: 513\n",
      "BATCH: 514\n",
      "BATCH: 515\n",
      "BATCH: 516\n",
      "BATCH: 517\n",
      "BATCH: 518\n",
      "BATCH: 519\n",
      "BATCH: 520\n",
      "BATCH: 521\n",
      "BATCH: 522\n",
      "BATCH: 523\n",
      "BATCH: 524\n",
      "BATCH: 525\n",
      "BATCH: 526\n",
      "BATCH: 527\n",
      "BATCH: 528\n",
      "BATCH: 529\n",
      "BATCH: 530\n",
      "BATCH: 531\n",
      "BATCH: 532\n",
      "BATCH: 533\n",
      "BATCH: 534\n",
      "BATCH: 535\n",
      "BATCH: 536\n",
      "BATCH: 537\n",
      "BATCH: 538\n",
      "BATCH: 539\n",
      "BATCH: 540\n",
      "BATCH: 541\n",
      "BATCH: 542\n",
      "BATCH: 543\n",
      "BATCH: 544\n",
      "BATCH: 545\n",
      "BATCH: 546\n",
      "BATCH: 547\n",
      "BATCH: 548\n",
      "BATCH: 549\n",
      "BATCH: 550\n",
      "BATCH: 551\n",
      "BATCH: 552\n",
      "BATCH: 553\n",
      "BATCH: 554\n",
      "BATCH: 555\n",
      "BATCH: 556\n",
      "BATCH: 557\n",
      "BATCH: 558\n",
      "BATCH: 559\n",
      "BATCH: 560\n",
      "BATCH: 561\n",
      "BATCH: 562\n",
      "BATCH: 563\n",
      "BATCH: 564\n",
      "BATCH: 565\n",
      "BATCH: 566\n",
      "BATCH: 567\n",
      "BATCH: 568\n",
      "BATCH: 569\n",
      "BATCH: 570\n",
      "BATCH: 571\n",
      "BATCH: 572\n",
      "BATCH: 573\n",
      "BATCH: 574\n",
      "BATCH: 575\n",
      "BATCH: 576\n",
      "BATCH: 577\n",
      "BATCH: 578\n",
      "BATCH: 579\n",
      "BATCH: 580\n",
      "BATCH: 581\n",
      "BATCH: 582\n",
      "BATCH: 583\n",
      "BATCH: 584\n",
      "BATCH: 585\n"
     ]
    }
   ],
   "source": [
    "for batch in range(BATCHES):\n",
    "    batch_data = training_sents[batch * BATCH_SIZE : (batch + 1) * BATCH_SIZE]\n",
    "    question_ids = batcher.batch_sentences(batch_data)\n",
    "    #question_ids = batcher.batch_sentences(training_sents[idx: idx+1])\n",
    "    elmo_question_input_ = sess.run(elmo_question_input['weighted_op'], \n",
    "                                    feed_dict={question_character_ids: question_ids})\n",
    "    shape = elmo_question_input_.shape\n",
    "    output_array[batch * BATCH_SIZE : (batch + 1) * BATCH_SIZE, :shape[1], :shape[2]] = elmo_question_input_\n",
    "    #if batch % 100 == 0:\n",
    "    print(\"BATCH: %d\" % batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_array[0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_array1 = np.copy(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(training_sents)):\n",
    "    output_array1[idx, len(training_sents[idx]):, :] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Users/yqk798/dev/notebooks/data/ner/combined/train-1.npy', output_array1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected dimension <= 2 array or matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-529f9c706839>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msparse_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_array1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expected dimension <= 2 array or matrix'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected dimension <= 2 array or matrix"
     ]
    }
   ],
   "source": [
    "sparse_embeddings = sparse.coo_matrix(output_array1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
